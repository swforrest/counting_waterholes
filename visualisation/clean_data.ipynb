{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from counting_boats.boat_utils.spatial_helpers import get_array_from_tif, use_udm_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "UDM_DIR = \"D:\\\\Results\\\\UDM\"\n",
    "DETECTION_FILE = \"C:\\\\ML_Software\\\\All_Results\\\\boat_detections.csv\"\n",
    "GEOJSON = \"C:\\\\ML_Software\\\\data\\\\moreton-geojson-format.geojson\"\n",
    "print(locals().get('removed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all the results \n",
    "DIR = \"C:\\\\ML_Software\\\\All_Results\"\n",
    "# get all folders in the folder\n",
    "folders = [f for f in os.listdir(DIR) if os.path.isdir(os.path.join(DIR, f))]\n",
    "print(folders)\n",
    "detections = pd.DataFrame()\n",
    "coverage = pd.DataFrame()\n",
    "orders = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    print(folder)\n",
    "    if folder == \"UDM\": continue\n",
    "    path = os.path.join(DIR, folder)\n",
    "    new_detections = pd.read_csv(os.path.join(path, \"boat_detections.csv\"))\n",
    "    new_detections['date'] = pd.to_datetime(new_detections['date'], dayfirst=True)\n",
    "    new_coverage = pd.read_csv(os.path.join(path, \"coverage.csv\"))\n",
    "    new_coverage['date'] = pd.to_datetime(new_coverage['date'])\n",
    "    new_orders = pd.read_csv(os.path.join(path, \"orders.csv\"))\n",
    "    new_orders['date'] = pd.to_datetime(new_orders['date'])\n",
    "    # merge the dataframes - \n",
    "    # Important: check if any rows are duplicates i.e new_orders contains an order\n",
    "    # that is already in the orders dataframe\n",
    "    duplicate_dates = pd.DataFrame()\n",
    "    if not orders.empty:\n",
    "        duplicate_dates = new_orders[new_orders['date'].isin(orders['date'])]\n",
    "    if not duplicate_dates.empty:\n",
    "        print(f\"Duplicate orders found in {folder}\")\n",
    "        print(duplicate_dates['date'])\n",
    "        # need to remove the duplicates\n",
    "        new_orders = new_orders[~new_orders['date'].isin(duplicate_dates['date'])]\n",
    "        # also remove the corresponding detections and coverage\n",
    "        new_detections = new_detections[~new_detections['date'].isin(duplicate_dates['date'])]\n",
    "        new_coverage = new_coverage[~new_coverage['date'].isin(duplicate_dates['date'])]\n",
    "    detections = pd.concat([detections, new_detections], ignore_index=True)\n",
    "    coverage = pd.concat([coverage, new_coverage], ignore_index=True)\n",
    "    orders = pd.concat([orders, new_orders], ignore_index=True)\n",
    "\n",
    "# sort the dataframes by date\n",
    "detections = detections.sort_values(by='date')\n",
    "coverage = coverage.sort_values(by='date')\n",
    "orders = orders.sort_values(by='date')\n",
    "\n",
    "# change date back to \"dd/mm/yyyy\"\n",
    "detections['date'] = detections['date'].dt.strftime('%d/%m/%Y')\n",
    "coverage['date'] = coverage['date'].dt.strftime('%d/%m/%Y')\n",
    "orders['date'] = orders['date'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# save the dataframes\n",
    "detections.to_csv(os.path.join(DIR, \"boat_detections.csv\"), index=False)\n",
    "coverage.to_csv(os.path.join(DIR, \"coverage.csv\"), index=False)\n",
    "orders.to_csv(os.path.join(DIR, \"orders.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1049407 detectionss\n"
     ]
    }
   ],
   "source": [
    "# Removes point outside the geojson\n",
    "\n",
    "# import json\n",
    "# from shapely.geometry import shape, Point\n",
    "\n",
    "# with open(GEOJSON) as f:\n",
    "#     gj = json.load(f)\n",
    "\n",
    "# polygons = []\n",
    "# for feature in gj['features']:\n",
    "#     polygon = shape(feature['geometry'])\n",
    "#     polygons.append(polygon)\n",
    "\n",
    "# ids_to_remove = []\n",
    "# detections = pd.read_csv(DETECTION_FILE)\n",
    "\n",
    "# for i, row in detections.iterrows():\n",
    "#     point = Point(row['longitude'], row['latitude'])\n",
    "#     for j, polygon in enumerate(polygons):\n",
    "#         if polygon.contains(point):\n",
    "#             ids_to_remove.append(i)\n",
    "#             break\n",
    "#     if i % 1000 == 0:\n",
    "#         print(f\"Processed {i} detections\", end=\"\\r\")\n",
    "    \n",
    "# print(f\"Removing {len(ids_to_remove)} detections\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 1049407/1060870 detections to C:\\ML_Software\\All_Results\\boat_detections.csv\n"
     ]
    }
   ],
   "source": [
    "# ids_to_keep = np.array(ids_to_remove) # actually we want to keep these ones\n",
    "\n",
    "# detections_keep = detections.iloc[ids_to_keep]\n",
    "\n",
    "# print(f\"Writing {len(detections_keep)}/{len(detections)} detections to {DETECTION_FILE}\")\n",
    "\n",
    "# detections_keep.to_csv(DETECTION_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Usable Data Mask, we want to:\n",
    "- Check if it is udm_2, if not then skip for now\n",
    "- Grab the cloud mask \n",
    "- Grab the detections for that day\n",
    "- filter out detections that are in the cloud mask -> false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\turner30\\AppData\\Local\\Temp\\ipykernel_25100\\3183592668.py:42: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  detections[\"date\"] = pd.to_datetime(detections[\"date\"], dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1049407 detections\n",
      "Preparing to process 2378 UDM files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2378/2378 [13:58<00:00,  2.84it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "CLOUD_BAND = 6\n",
    "HAZE_BAND = 4\n",
    "RESOLUTION = 3\n",
    "\n",
    "\n",
    "def process_udm(udm_data, day_detections, day):\n",
    "    \"\"\"\n",
    "    Using the mask, get the ids of detections that have x and y within the mask\n",
    "    \"\"\"\n",
    "    day_detections = day_detections[day_detections[\"date\"] == day]\n",
    "\n",
    "    mask, top_x, top_y = udm_data\n",
    "\n",
    "    clear = mask\n",
    "    H, W = clear.shape\n",
    "\n",
    "    items_x = day_detections[\"epsg32756_x\"]\n",
    "    items_y = day_detections[\"epsg32756_y\"]\n",
    "\n",
    "    delta_x = items_x - top_x\n",
    "    delta_y = top_y - items_y\n",
    "\n",
    "    pixel_i = np.round(delta_y / RESOLUTION).astype(int)\n",
    "    pixel_j = np.round(delta_x / RESOLUTION).astype(int)\n",
    "\n",
    "    within_bounds = (pixel_i >= 0) & (pixel_i < H) & (pixel_j >= 0) & (pixel_j < W)\n",
    "\n",
    "    pixel_i = pixel_i[within_bounds]\n",
    "    pixel_j = pixel_j[within_bounds]\n",
    "\n",
    "    clear_values = clear[pixel_i, pixel_j] == 1\n",
    "\n",
    "    filtered_ids = day_detections[within_bounds][clear_values].index\n",
    "    # print(f\"Found {len(filtered_ids)}/{len(day_detections)} detections for {day}. First 5: {filtered_ids[:5]}\")\n",
    "\n",
    "    return filtered_ids\n",
    "\n",
    "\n",
    "detections = pd.read_csv(DETECTION_FILE)\n",
    "detections[\"date\"] = pd.to_datetime(detections[\"date\"], dayfirst=True)\n",
    "print(f\"Loaded {len(detections)} detections\")\n",
    "\n",
    "all_udm = [udm for udm in os.listdir(UDM_DIR) if udm.endswith(\".npz\")]\n",
    "\n",
    "print(f\"Preparing to process {len(all_udm)} UDM files\")\n",
    "\n",
    "all_valid_ids = []\n",
    "\n",
    "if os.path.exists(\"completed.npy\"):\n",
    "    completed = list(np.load(\"completed.npy\"))\n",
    "    all_valid_ids = list(np.load(\"valid_ids.npy\"))\n",
    "else:\n",
    "    completed = []\n",
    "\n",
    "\n",
    "for udm in tqdm(all_udm):\n",
    "    if udm in completed:\n",
    "        continue\n",
    "    udm_path = os.path.join(UDM_DIR, udm)\n",
    "    data = np.load(udm_path)\n",
    "    mask = data[\"mask\"]\n",
    "    minx = data[\"minx\"].item()\n",
    "    miny = data[\"miny\"].item()\n",
    "    maxx = data[\"maxx\"].item()\n",
    "    maxy = data[\"maxy\"].item()\n",
    "    resx = data[\"resx\"].item()\n",
    "    resy = data[\"resy\"].item()\n",
    "    date = udm.split(\"_\")[1] # date is in the format \"YYYYMMDD\"\n",
    "    date = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "    valid_ids = process_udm((mask, minx, maxy), detections[detections['date'] == date], date)\n",
    "    all_valid_ids.extend(valid_ids)\n",
    "    completed.append(udm)\n",
    "    np.save(\"completed.npy\", completed)\n",
    "    np.save(\"valid_ids.npy\", all_valid_ids)\n",
    "\n",
    "filtered_detection_ids = np.array(all_valid_ids)\n",
    "filtered_detections = detections.iloc[filtered_detection_ids]\n",
    "filtered_detections.to_csv(DETECTION_FILE + \".filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10775688\n"
     ]
    }
   ],
   "source": [
    "print(filtered_detections.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Boats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
